{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a8eab6",
   "metadata": {},
   "source": [
    "Given the names and grades for each student in a class of  students, store them in a nested list and print the name(s) of any student(s) having the second lowest grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5966b0a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'proxies_gen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_76992/3260403322.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0muser_agent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate_user_agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mproxies_gen\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_proxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_proxies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'proxies_gen'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, element\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import unidecode\n",
    "from user_agent import generate_user_agent\n",
    "from proxies_gen import get_proxies, test_proxies\n",
    "from itertools import cycle\n",
    "from lxml.html import fromstring\n",
    "from multiprocessing import Pool, cpu_count  # This is a thread-based Pool\n",
    "from requests.exceptions import ConnectionError, Timeout, ProxyError, RequestException\n",
    "from urllib3.exceptions import ProtocolError\n",
    "import sys\n",
    "import os\n",
    "sys.setrecursionlimit(10000)  # need to optimize code.\n",
    "proxy_enabled = True\n",
    "\n",
    "\n",
    "def parse_games(game_tags):\n",
    "    \"\"\"\n",
    "    parse the games table on current page\n",
    "    parameters:\n",
    "    game_tags: games tags after reading the html page\n",
    "    df: the dataframe where we will store the games\n",
    "    \"\"\"\n",
    "    global rec_count\n",
    "    global df\n",
    "    for tag in game_tags:\n",
    "        game = {}\n",
    "        game[\"Name\"] = \" \".join(tag.string.split())\n",
    "        #print(rec_count+1, 'Fetch Data for game', unidecode.unidecode(game['Name']))\n",
    "\n",
    "        data = tag.parent.parent.find_all(\"td\")\n",
    "        if data:\n",
    "            game[\"Rank\"] = np.int32(data[0].string)\n",
    "            game[\"img_url\"] = data[1].a.img.get('src')\n",
    "            game[\"url\"] = data[2].a.get('href')\n",
    "            if len(game[\"Name\"].split(\"/\")) > 1:\n",
    "                # replace accented chars with ascii\n",
    "                game[\"basename\"] = unidecode.unidecode(\n",
    "                    game['Name'].strip().split('/')[0].strip().replace(' ', '-'))\n",
    "            else:\n",
    "                game[\"basename\"] = game[\"url\"].rsplit('/', 2)[1]\n",
    "            game[\"Platform\"] = data[3].img.get('alt')\n",
    "            game[\"Publisher\"] = data[4].get_text().strip()\n",
    "            game[\"Developer\"] = data[5].get_text().strip()\n",
    "            game[\"Vgchartzscore\"] = data[6].get_text().strip()\n",
    "            game[\"Critic_Score\"] = float(\n",
    "                data[7].string) if not data[7].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"User_Score\"] = float(\n",
    "                data[8].string) if not data[8].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"Total_Shipped\"] = float(\n",
    "                data[9].string[:-1]) if not data[9].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"Global_Sales\"] = float(\n",
    "                data[10].string[:-1]) if not data[10].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"NA_Sales\"] = float(\n",
    "                data[11].string[:-1]) if not data[11].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"EU_Sales\"] = float(\n",
    "                data[12].string[:-1]) if not data[12].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"JP_Sales\"] = float(\n",
    "                data[13].string[:-1]) if not data[13].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"Other_Sales\"] = float(\n",
    "                data[14].string[:-1]) if not data[14].string.startswith(\"N/A\") else np.nan\n",
    "            year = data[15].string.split()[-1]\n",
    "            if year.startswith('N/A'):\n",
    "                game[\"Year_of_Release\"] = 'N/A'\n",
    "            else:\n",
    "                if int(year) >= 70:\n",
    "                    year_to_add = np.int32(\"19\" + year)\n",
    "                else:\n",
    "                    year_to_add = np.int32(\"20\" + year)\n",
    "                game[\"Year_of_Release\"] = year_to_add\n",
    "            game[\"Last_Update\"] = data[16].get_text().strip()\n",
    "            game['Genre'] = 'N/A'\n",
    "            game['Rating'] = 'N/A'\n",
    "            game['status'] = 0\n",
    "            df = df.append(game, ignore_index=True)\n",
    "        rec_count += 1\n",
    "\n",
    "\n",
    "def parse_genre_esrb(df):\n",
    "    \"\"\"loads every game's url to get genre and esrb rating\"\"\"\n",
    "    headers = {'User-Agent': generate_user_agent(\n",
    "        device_type='desktop', os=('mac', 'linux'))}\n",
    "    proxy = {}\n",
    "    if proxy_enabled:\n",
    "        #print(\"\\n******getting list of proxies and testing them******'\\n\")\n",
    "        # this an api call which returns a list of working proxies that get checked evrey 15 minutes\n",
    "        proxies = cycle(get_proxies(5))\n",
    "        proxy = next(proxies)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            game_page = requests.get(df.at[index, 'url'], headers=headers, proxies={\"http\": proxy, \"https\": proxy}, timeout=5)\n",
    "            if game_page.status_code == 200:\n",
    "                sub_soup = BeautifulSoup(game_page.text, \"lxml\")\n",
    "                # again, the info box is inconsistent among games so we\n",
    "                # have to find all the h2 and traverse from that to the genre\n",
    "                gamebox = sub_soup.find(\"div\", {\"id\": \"gameGenInfoBox\"})\n",
    "                h2s = gamebox.find_all('h2')\n",
    "                # make a temporary tag here to search for the one that contains\n",
    "                # the word \"Genre\"\n",
    "                temp_tag = element.Tag\n",
    "                for h2 in h2s:\n",
    "                    if h2.string == 'Genre':\n",
    "                        temp_tag = h2\n",
    "                df.loc[index, 'Genre'] = temp_tag.next_sibling.string\n",
    "\n",
    "                # find the ESRB rating\n",
    "                game_rating = gamebox.find('img').get('src')\n",
    "                if 'esrb' in game_rating:\n",
    "                    df.loc[index, 'Rating'] = game_rating.split(\n",
    "                        '_')[1].split('.')[0].upper()\n",
    "                # we successfuly got the genre and rating\n",
    "                df.loc[index, 'status'] = 1\n",
    "                #print('Successfully scraped genre and rating for :', df.at[index, 'Name'])\n",
    "\n",
    "        except(ProxyError):\n",
    "            proxy = next(proxies)\n",
    "\n",
    "        except (ConnectionError, Timeout, ProtocolError, TimeoutError):\n",
    "            #print('Something went wrong while connecting to', df.at[index, 'Name'], 'url, will try again later')\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            #print('different error occurred while connecting, will pass')\n",
    "            continue\n",
    "        # wait for 1 seconds between every call,\n",
    "        # we do not want to get blocked or abuse the server\n",
    "        time.sleep(1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def retry_game(df):\n",
    "    \"\"\"try to scrape the missing data again\"\"\"\n",
    "    return parse_genre_esrb(df)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def process_games(df):\n",
    "        failed_games = len(df[df['status'] == 0])\n",
    "        NUM_WORKERS = cpu_count() * 2\n",
    "        df_subsets = np.array_split(df[df['status'] == 0], NUM_WORKERS)\n",
    "        #update num_workers\n",
    "        df_subsets = [i for i in df_subsets if len(i) != 0]\n",
    "        if len(df_subsets) != 0:\n",
    "            NUM_WORKERS = len(df_subsets)# we don't want to have a worker for empty subsets\n",
    "            pool = Pool(processes=NUM_WORKERS)\n",
    "            results = pool.map(retry_game, df_subsets)\n",
    "            try:\n",
    "                df_updated = pd.concat(results)\n",
    "                df = pd.concat([df[df['status'] == 1], df_updated])\n",
    "            except: \n",
    "                print('error occurred while joining dataframe')\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        return df\n",
    "\n",
    "    rec_count = 0\n",
    "    start_time = time.time()\n",
    "    current_time = time.time()\n",
    "    crashed_tag = 'before_crashing_'\n",
    "    exists = [s for s in os.listdir() if crashed_tag in s]\n",
    "    if exists:\n",
    "        print(\"found a data saved from a crash, will continue on it\")\n",
    "        csvfilename = exists[0].replace(crashed_tag, '')\n",
    "        df = pd.read_csv(exists[0])\n",
    "        rec_count = df['Rank'].max()\n",
    "        page = int(rec_count/1000) + 1 # because we already scraped current \n",
    "        df = process_games(df)\n",
    "    else:\n",
    "        csvfilename = \"vgsales-\" + time.strftime(\"%Y-%m-%d_%H_%M_%S\") + \".csv\"\n",
    "\n",
    "    # initialize a panda dataframe to store all games with the following columns:\n",
    "    # rank, name, img-url, vgchartz score, genre, ESRB rating, platform, developer,\n",
    "    # publisher, release year, critic score, user score, na sales, pal sales,\n",
    "    # jp sales, other sales, total sales, total shipped, last update, url, status\n",
    "    # last two columns for debugging\n",
    "    if not exists:\n",
    "        df = pd.DataFrame(columns=[\n",
    "            'Rank', 'Name', 'basename', 'Genre', 'Rating', 'Platform', 'Publisher',\n",
    "            'Developer', 'VGChartz_Score', 'Critic_Score', 'User_Score',\n",
    "            'Total_Shipped', 'Global_Sales', 'NA_Sales', 'EU_Sales', 'JP_Sales',\n",
    "            'Other_Sales', 'Year_of_Release', 'Last_Update', 'url', 'status'])\n",
    "\n",
    "    urlhead = 'http://www.vgchartz.com/games/games.php?page='\n",
    "    urltail = '&results=1000&name=&console=&keyword=&publisher=&genre=&order=Sales&ownership=Both'\n",
    "    urltail += '&banner=Both&showdeleted=&region=All&goty_year=&developer='\n",
    "    urltail += '&direction=DESC&showtotalsales=1&shownasales=1&showpalsales=1&showjapansales=1'\n",
    "    urltail += '&showothersales=1&showpublisher=1&showdeveloper=1&showreleasedate=1&showlastupdate=1'\n",
    "    urltail += '&showvgchartzscore=1&showcriticscore=1&showuserscore=1&showshipped=1&alphasort=&showmultiplat=Yes&showgenre=1'\n",
    "\n",
    "    # get the number of pages\n",
    "    vglink = requests.get('http://www.vgchartz.com/gamedb/').text\n",
    "    x = fromstring(vglink).xpath(\n",
    "        \"//th[@colspan='3']/text()\")[0].split('(', 1)[1].split(')')[0]\n",
    "    pages = int(x.split(',')[0])\n",
    "\n",
    "    if not exists: page = 1\n",
    "    while True:\n",
    "        if page > pages:\n",
    "            break\n",
    "        try:\n",
    "            proxy = get_proxies(1)[0]\n",
    "            headers = {'User-Agent': generate_user_agent(\n",
    "                device_type='desktop', os=('mac', 'linux'))}\n",
    "            surl = urlhead + str(page) + urltail\n",
    "            r = requests.get(surl, headers=headers, proxies={\n",
    "                            'http': proxy, 'https': proxy}, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                soup = BeautifulSoup(r.text, 'lxml')\n",
    "                print(\"******Scraping page \" + str(page) + \"******'\\n\")\n",
    "\n",
    "                # vgchartz website is really weird so we have to search for\n",
    "                # <a> tags with game urls\n",
    "                game_tags = list(filter(\n",
    "                    lambda x: x.attrs['href'].startswith('http://www.vgchartz.com/game/'), soup.find_all(\"a\")))[10:]\n",
    "                # discard the first 10 elements because those\n",
    "                # links are in the navigation bar\n",
    "\n",
    "                parse_games(game_tags)\n",
    "                page += 1\n",
    "                print('\\n******begin scraping for Genre and Rating******\\n')\n",
    "                df = process_games(df)\n",
    "\n",
    "        except (ConnectionError, Timeout, ProxyError, RequestException, ProtocolError, TimeoutError):\n",
    "            print('Something went wrong while connecting to page: ',\n",
    "                page, ', will try again later')\n",
    "            #proxy = get_proxies(1)\n",
    "            time.sleep(10)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"something went wrong! We're on page: \" +\n",
    "                str(page) + '\\nSaving successfully crawled data')\n",
    "            print(\"Exception: \", e)\n",
    "            df.to_csv(crashed_tag + csvfilename, sep=\",\",\n",
    "                    encoding='utf-8', index=False)\n",
    "            raise e\n",
    "\n",
    "\n",
    "    failed_games = len(df[df['status'] == 0])\n",
    "    print(\"******Finished scraping games, will try to scrape missing data******\")\n",
    "    # 36 hours max, should be enough to scrape everything\n",
    "    t_end = start_time + 60 * 60 * 36\n",
    "    while True:\n",
    "        try:\n",
    "            df = process_games(df)\n",
    "            failed_games = len(df[df['status'] == 0])\n",
    "            if failed_games == 0 or time.time() > t_end:\n",
    "                break\n",
    "            #print('Number of not scraped yet:', failed_games, '\\n')\n",
    "            time.sleep(10)  # wait for 10 seconds for the server to recover?\n",
    "        except Exception as e:\n",
    "            print(\"something went wrong! We're on page: \" + str(page) + '\\nSaving successfully crawled data')\n",
    "            print(\"Exception: \", e)\n",
    "            df.to_csv(crashed_tag + csvfilename, sep=\",\",\n",
    "                    encoding='utf-8', index=False)\n",
    "            raise e\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Scraped\", rec_count, \"games in\", round(elapsed_time/60, 2), \"minutes.\")\n",
    "\n",
    "    # select only these columns in the final dataset\n",
    "    df = df.sort_values(by=['Rank'])\n",
    "    df.to_csv('complete-vgchartz.csv', sep=\",\", encoding='utf-8', index=False)\n",
    "    df_final = df[[\n",
    "        'Rank', 'Name', 'Platform', 'Year_of_Release', 'Genre', 'Rating',\n",
    "        'Publisher', 'Developer', 'Critic_Score', 'User_Score',\n",
    "        'Global_Sales', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']]\n",
    "\n",
    "    df_final.to_csv(csvfilename, sep=\",\", encoding='utf-8', index=False)\n",
    "    print(\"Wrote scraper data to\", csvfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56e081ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (Temp/ipykernel_76992/3954549129.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\janat\\AppData\\Local\\Temp/ipykernel_76992/3954549129.py\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    proxies.append(proxy)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from lxml.html import fromstring\n",
    "import requests\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "def get_proxies(num=None):\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = list(requests.get('https://proxy.rudnkh.me/txt').text.split())\n",
    "    for i in parser.xpath('//tbody/tr'):\n",
    "         if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "                 proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "                 proxies.append(proxy)\n",
    "\n",
    "    link = \"https://api.proxyscrape.com/?request=getproxies&proxytype=http&timeout=1000&country=all&ssl=all&anonymity=all&uptime=100\"\n",
    "    proxies = list(requests.get(link).text.split())\n",
    "    np.random.shuffle(proxies)\n",
    "    proxies = []\n",
    "    if len(proxies) == 0:\n",
    "        proxies = list(requests.get(\n",
    "            link[:-3]+'99').text.split())  # change uptime to 99\n",
    "        np.random.shuffle(proxies)\n",
    "    # print('Found', len(proxies), 'proxies, testing them now')\n",
    "\n",
    "    if num is None:\n",
    "        num = len(proxies)\n",
    "    tested = test_proxies(proxies, num)\n",
    "    return tested\n",
    "\n",
    "\n",
    "def test_proxies(proxies, num):\n",
    "    url = 'https://httpbin.org/ip'\n",
    "    proxy_pool = cycle(proxies)\n",
    "    working_proxies = []\n",
    "    for i in range(1, len(proxies)):\n",
    "        if num == 0:\n",
    "            break\n",
    "        # Get a proxy from the pool\n",
    "        proxy = next(proxy_pool)\n",
    "        # print(\"Request #%d\" % i)\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                url, proxies={\"http\": proxy, \"https\": proxy}, timeout=1)\n",
    "            # print(response.json())\n",
    "            working_proxies.append(proxy)\n",
    "            num -= 1\n",
    "        except:\n",
    "            # Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work.\n",
    "            # print(\"Skipping. Connnection error\")\n",
    "            pass\n",
    "    return working_proxies\n",
    "\n",
    "\n",
    "proxies = get_proxies(5)\n",
    "with open('proxies.txt') as f:\n",
    "        proxies = f.read().splitlines()\n",
    "test_proxies(proxies, 10)\n",
    "print(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1fce450",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_76992/1217391064.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;31m# Run the code to scrape! I did 10,000 rows per page to speed things up.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscrape_all_vg_chartz_videogame_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;31m# Compress and store for later!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_76992/1217391064.py\u001b[0m in \u001b[0;36mscrape_all_vg_chartz_videogame_db\u001b[1;34m(results_per_page)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mshowpublisher\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m&\u001b[0m\u001b[0mshowdeveloper\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m&\u001b[0m\u001b[0mshowreleasedate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m&\u001b[0m\u001b[0mshowlastupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m&\u001b[0m\u001b[0mshowvgchartzscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m&\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mshowcriticscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m&\u001b[0m\u001b[0mshowuserscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m&\u001b[0m\u001b[0mshowshipped\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m&\u001b[0m\u001b[0malphasort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m&\u001b[0m\u001b[0mshowmultiplat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNo\u001b[0m\u001b[0;31m'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mnew_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_vgchartz_videogame_db_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_76992/1217391064.py\u001b[0m in \u001b[0;36mscrape_vgchartz_videogame_db_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mpage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Selects the table with all the data in it on HTML using xpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lh' is not defined"
     ]
    }
   ],
   "source": [
    "def find_console_tags(soup):\n",
    "    # Console tags are stored as images, so we find the image tag and record its 'alt' value as text\n",
    "    consoles = list()\n",
    "    for img in soup.find_all('img'):\n",
    "        if 'images/consoles'in img['src']:\n",
    "            # Cut file path elements from string\n",
    "            console_tag = (img['src'][17:-6])\n",
    "            consoles.append(img['alt'])\n",
    "    return consoles\n",
    "\n",
    "\n",
    "# Find the names of games from the links\n",
    "def find_names_column(table_path):\n",
    "    names_list = list()\n",
    "    for row in table_path.xpath('.//tr'):\n",
    "        for td in row.xpath('.//td'):\n",
    "            if not td.find('a') is None:\n",
    "                names_list.append(td.find('a').text.strip()) \n",
    "    return names_list\n",
    "\n",
    "# Write a function that takes in a VGChartz URL and gives us all the data in their video game database\n",
    "def scrape_vgchartz_videogame_db_page(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "\n",
    "    ### Check the Status\n",
    "    assert(response.status_code == 200),\" Website not OK \" # status code = 200 => OK\n",
    "    \n",
    "    #Store the contents of the website under doc\n",
    "    page=response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    doc = lh.fromstring(response.content)\n",
    "    \n",
    "    # Selects the table with all the data in it on HTML using xpath\n",
    "    target_table_path = doc.xpath('//*[@id=\"generalBody\"]/table')[0]\n",
    "\n",
    "    # Find column values that won't be scraped correctly with .text option\n",
    "    names_list = find_names_column(target_table_path)\n",
    "    consoles = find_console_tags(soup)\n",
    "    \n",
    "    # Parse non-image and non-URL info from the data table to a pandas DataFrame\n",
    "    row_dict={}\n",
    "    df=pd.DataFrame()\n",
    "    row_list= list()\n",
    "    for counter,row in enumerate(target_table_path.xpath(\".//tr\")):\n",
    "        if counter > 2: # To skip header rows\n",
    "            row_list=[td.text for td in row.xpath(\".//td\")]\n",
    "            row_dict[counter] = row_list\n",
    "\n",
    "    df=pd.DataFrame.from_dict(row_dict).transpose()\n",
    "    df.columns = ['position','game','blank','console','publisher','developer','vgchart_score',\\\n",
    "                 'critic_score','user_score','total_shipped','total_sales',\\\n",
    "                  'na_sales','pal_sales','japan_sales','other_sales',\\\n",
    "                  'release_date','last_update']\n",
    "    \n",
    "    # Correct the console and game columns using scraped values\n",
    "    \n",
    "    df=df.reset_index().drop(columns = ['index','blank'])\n",
    "    df['console'] = consoles\n",
    "    df['game'] = names_list\n",
    "    return df\n",
    "\n",
    "    # We can 'hack' the URL to display any number of results per page. I'll leave it as an argument.\n",
    "def scrape_all_vg_chartz_videogame_db(results_per_page):\n",
    "    df = pd.DataFrame()\n",
    "    current_page = 1\n",
    "    games_left = True\n",
    "    while games_left:\n",
    "        url = 'http://www.vgchartz.com/games/games.php?page=' + str(current_page) +\\\n",
    "        '&results=' + str(results_per_page) + '&name=&console=&keyword=&publisher=&genre=&order=Sales&ownership\\\n",
    "        =Both&boxart=Both&banner=Both&showdeleted=&region=All&goty_year=&developer=&direction\\\n",
    "        =DESC&showtotalsales=1&shownasales=1&showpalsales=1&showjapansales=1&showothersales=1&\\\n",
    "        showpublisher=1&showdeveloper=1&showreleasedate=1&showlastupdate=1&showvgchartzscore=1&\\\n",
    "        showcriticscore=1&showuserscore=1&showshipped=1&alphasort=&showmultiplat=No'\n",
    "        new_df = scrape_vgchartz_videogame_db_page(url)\n",
    "        df = df.append(new_df)\n",
    "\n",
    "# Run the code to scrape! I did 10,000 rows per page to speed things up.\n",
    "df=scrape_all_vg_chartz_videogame_db(10000)\n",
    "\n",
    "# Compress and store for later!\n",
    "df.to_csv(vgsales.csv, sep=\",\", encoding='utf-8', index=False)\n",
    "print(\"Wrote scraper data to\", csvfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "414bacd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 18:28:19 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-09-22 18:28:19 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1o  3 May 2022), cryptography 3.4.8, Platform Windows-10-10.0.22000-SP0\n",
      "2022-09-22 18:28:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30}\n",
      "2022-09-22 18:28:23 [py.warnings] WARNING: C:\\Users\\janat\\anaconda\\lib\\site-packages\\scrapy\\extensions\\feedexport.py:289: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n",
      "2022-09-22 18:28:34 [py.warnings] WARNING: C:\\Users\\janat\\anaconda\\lib\\site-packages\\scrapy\\core\\scraper.py:157: UserWarning: The \"VGSpider.parse\" method is a generator and includes a \"return\" statement with a value different than None. This could lead to unexpected behaviour. Please see https://docs.python.org/3/reference/simple_stmts.html#the-return-statement for details about the semantics of the \"return\" statement within generators\n",
      "  warn_on_generator_with_return_value(spider, callback)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\n",
      "Adventure\n",
      "Action-Adventure\n",
      "Board+Game\n",
      "Education\n",
      "Fighting\n",
      "Misc\n",
      "MMO\n",
      "Music\n",
      "Party\n",
      "Platform\n",
      "Puzzle\n",
      "Racing\n",
      "Role-Playing\n",
      "Sandbox\n",
      "Shooter\n",
      "Simulation\n",
      "Sports\n",
      "Strategy\n",
      "Visual+Novel\n",
      "It took 1529.4618041999997 seconds to retrieve this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 18:53:55 [py.warnings] WARNING: C:\\Users\\janat\\AppData\\Local\\Temp/ipykernel_76992/1952637497.py:292: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  games[\"genre\"] = games[\"genre\"].str.replace(\"+\", \" \")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61975 game records retrieved.\n",
      "File saved as vgchartz-9_22_2022.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date\n",
    "import re\n",
    "import math\n",
    "import scrapy\n",
    "import json\n",
    "import logging\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Here is our JSON writer pipeline\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    # When the spider is open, it writes itself to the gamesresult to a julia file\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('gamesresult.jl', 'w')\n",
    "\n",
    "    # When the spider closes, it closes that file as it is done writing to it\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # This function dictates how the spider writes to the .jl file\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "\n",
    "# I'll just create a string for today's date to append to the end of the names of the files we create\n",
    "today_date_string = str(date.today().month) + \"_\" + str(date.today().day) + \"_\" + str(date.today().year)\n",
    "\n",
    "page = 2 # The first page we will need to jump to is page number 2, so this is that variable\n",
    "genre = 0 # We are starting at the first genre in the list\n",
    "\n",
    "# The quick and admittedly dirty way to do this is if we want to know the genre of each game is to cycle through the urls with\n",
    "# each of the following genres as a way of changing the genre parameter of the web address\n",
    "\n",
    "genre_list = [\"Action\",\n",
    "             \"Adventure\",\n",
    "             \"Action-Adventure\",\n",
    "             \"Board+Game\",\n",
    "             \"Education\",\n",
    "             \"Fighting\",\n",
    "             \"Misc\",\n",
    "             \"MMO\",\n",
    "             \"Music\",\n",
    "             \"Party\",\n",
    "             \"Platform\",\n",
    "             \"Puzzle\",\n",
    "             \"Racing\",\n",
    "             \"Role-Playing\",\n",
    "             \"Sandbox\",\n",
    "             \"Shooter\",\n",
    "             \"Simulation\",\n",
    "             \"Sports\",\n",
    "             \"Strategy\",\n",
    "             \"Visual+Novel\"]\n",
    "\n",
    "class VGSpider(scrapy.Spider):\n",
    "    global genre\n",
    "\n",
    "    name = \"game_spider\"\n",
    "    start_urls = ['https://www.vgchartz.com/games/games.php?name=&keyword=&console=&region=All&developer=&publisher=&goty_year=&genre=' + genre_list[0] + '&boxart=Both&banner=Both&ownership=Both&showmultiplat=No&results=200&order=Sales&showtotalsales=0&showtotalsales=1&showpublisher=0&showpublisher=1&showvgchartzscore=0&showvgchartzscore=1&shownasales=0&shownasales=1&showdeveloper=0&showdeveloper=1&showcriticscore=0&showcriticscore=1&showpalsales=0&showpalsales=1&showreleasedate=0&showreleasedate=1&showuserscore=0&showuserscore=1&showjapansales=0&showjapansales=1&showlastupdate=0&showlastupdate=1&showothersales=0&showothersales=1&showshipped=0&showshipped=1']\n",
    "\n",
    "    # Here's where we set the logging settings.\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # This uses the functions in the JsonWriterPipeline class to write the\n",
    "                                                              # Julia file\n",
    "        'FEED_FORMAT':'json',                                 # This sets the feed exporter to export as a JSON file\n",
    "        'FEED_URI': \"gamesresult-\" + today_date_string + \".json\" # This simply sets the title for said JSON file\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        global genre\n",
    "        global page\n",
    "\n",
    "        # Here we declare the selector for each piece of data--this tells scrapy where to check for each item\n",
    "\n",
    "        IMAGE_SELECTOR = './/td[2]/div/a/div/img/@src'\n",
    "        TITLE_SELECTOR = './/td[3]/a/text()'\n",
    "        CONSOLE_SELECTOR = './/td[4]/img/@alt'\n",
    "        PUBLISHER_SELECTOR = './/td[5]/text()'\n",
    "        DEVELOPER_SELECTOR = './/td[6]/text()'\n",
    "        VGSCORE_SELECTOR = './/td[7]/text()'\n",
    "        CRITIC_SELECTOR = './/td[8]/text()'\n",
    "        USER_SELECTOR = './/td[9]/text()'\n",
    "        TOTALSHIPPED_SELECTOR = './/td[10]/text()'\n",
    "        TOTALSALES_SELECTOR = './/td[11]/text()'\n",
    "        NASALES_SELECTOR = './/td[12]/text()'\n",
    "        PALSALES_SELECTOR = './/td[13]/text()'\n",
    "        JPSALES_SELECTOR = './/td[14]/text()'\n",
    "        OTHER_SELECTOR = './/td[15]/text()'\n",
    "        RELEASEDATE_SELECTOR = './/td[16]/text()'\n",
    "        UPDATE_SELECTOR = './/td[17]/text()'\n",
    "\n",
    "        # We loop through each row (so each game) in the table, and snag the data we want, giving each one a name that make sense\n",
    "\n",
    "        for row in response.xpath('//*[@id=\"generalBody\"]/table[1]/tr'):\n",
    "            yield {\n",
    "\n",
    "                'img' : row.xpath(IMAGE_SELECTOR).extract(),\n",
    "                'title' : row.xpath(TITLE_SELECTOR).extract(),\n",
    "                'console' : row.xpath(CONSOLE_SELECTOR).extract(),\n",
    "                'publisher' : row.xpath(PUBLISHER_SELECTOR).extract(),\n",
    "                'developer' : row.xpath(DEVELOPER_SELECTOR).extract(),\n",
    "                'vg_score' : row.xpath(VGSCORE_SELECTOR).extract(),\n",
    "                'critic_score' : row.xpath(CRITIC_SELECTOR).extract(),\n",
    "                'user_score' : row.xpath(USER_SELECTOR).extract(),\n",
    "                'total_shipped' : row.xpath(TOTALSHIPPED_SELECTOR).extract(),\n",
    "                'total_sales' : row.xpath(TOTALSALES_SELECTOR).extract(),\n",
    "                'na_sales' : row.xpath(NASALES_SELECTOR).extract(),\n",
    "                'pal_sales' : row.xpath(PALSALES_SELECTOR).extract(),\n",
    "                'jp_sales' : row.xpath(JPSALES_SELECTOR).extract(),\n",
    "                'other_sales' : row.xpath(OTHER_SELECTOR).extract(),\n",
    "                'release_date' : row.xpath(RELEASEDATE_SELECTOR).extract(),\n",
    "                'last_update' : row.xpath(UPDATE_SELECTOR).extract(),\n",
    "                'genre' : genre_list[genre]\n",
    "            }\n",
    "\n",
    "        # next_page puts together--you guessed it--the next page. It uses the page number we established at the beginning\n",
    "        # and the genre parameter we're on currently.\n",
    "\n",
    "        next_page = \"https://www.vgchartz.com/games/games.php?page=\" + str(page) + \"&results=200&name=&console=&keyword=&publisher=&genre=\"+ genre_list[genre] + \"&order=Sales&ownership=Both&boxart=Both&banner=Both&showdeleted=&region=All&goty_year=&developer=&direction=DESC&showtotalsales=1&shownasales=1&showpalsales=1&showjapansales=1&showothersales=1&showpublisher=1&showdeveloper=1&showreleasedate=1&showlastupdate=1&showvgchartzscore=1&showcriticscore=1&showuserscore=1&showshipped=1&alphasort=&showmultiplat=No\"\n",
    "\n",
    "        # This selector will help us know just how far to go by grabbing the number of results for the given query total, not\n",
    "        # just on the page\n",
    "\n",
    "        RESULTS_SELECTOR = '//*[@id=\"generalBody\"]/table[1]/tr[1]/th[1]/text()'\n",
    "\n",
    "        # Below we grab that figure which is a string, and extract the numbers from it using a regular expression\n",
    "\n",
    "        results = response.xpath(RESULTS_SELECTOR).extract_first()\n",
    "        results_pat = r'([0-9]{1,9})'\n",
    "        results = results.replace(\",\", \"\")\n",
    "\n",
    "        # We then can divide that number by 200 (the number of results per page), and thusly figure out just how many pages we\n",
    "        # should cycle through before we've reached the end and should either switch genres or finish up.\n",
    "\n",
    "        last_page = math.ceil(int(re.search(results_pat, results).group(1)) / 200)\n",
    "\n",
    "        # Below we test whether we are at the last page and whether we've reached the last page of the last genre\n",
    "        # If we've reached the last page and we've reached the last genre (which according to our list is Visual Novel)\n",
    "        # we can end out spider and close up shop.\n",
    "\n",
    "        if (page > last_page) & (genre_list[genre] == \"Visual+Novel\"):\n",
    "            print(genre_list[genre])\n",
    "            return \"All done!\"\n",
    "\n",
    "        # If we've reached the last page, but not the last genre (anything BUT Visual Novel), we'll reset our page, move onto the\n",
    "        # next genre and keep scraping.\n",
    "        elif (page > last_page) & (genre_list[genre] != \"Visual+Novel\"):\n",
    "            print(genre_list[genre])\n",
    "            page = 1\n",
    "            genre += 1\n",
    "            next_page = \"https://www.vgchartz.com/games/games.php?page=\" + str(page) + \"&results=200&name=&console=&keyword=&publisher=&genre=\"+ genre_list[genre] + \"&order=Sales&ownership=Both&boxart=Both&banner=Both&showdeleted=&region=All&goty_year=&developer=&direction=DESC&showtotalsales=1&shownasales=1&showpalsales=1&showjapansales=1&showothersales=1&showpublisher=1&showdeveloper=1&showreleasedate=1&showlastupdate=1&showvgchartzscore=1&showcriticscore=1&showuserscore=1&showshipped=1&alphasort=&showmultiplat=No\"\n",
    "            yield scrapy.Request(\n",
    "                response.urljoin(next_page),\n",
    "                callback=self.parse\n",
    "                )\n",
    "            page += 1\n",
    "\n",
    "        # If we haven't reached the last page at all, we can just keep going without changing the genre parameter, we'll just have\n",
    "        # to move onto the next page\n",
    "        elif next_page:\n",
    "            yield scrapy.Request(\n",
    "                response.urljoin(next_page),\n",
    "                callback=self.parse\n",
    "                )\n",
    "            time.sleep(3)\n",
    "            page += 1\n",
    "\n",
    "# Here we finally simply create the process and set the spider we've created to crawl away\n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess(get_project_settings())\n",
    "    start = timer()\n",
    "    process.crawl(VGSpider)\n",
    "    process.start(stop_after_crawl=True) # Blocks here until the crawl is finished\n",
    "    end = timer()\n",
    "    print(\"It took \" + str(end - start) + \" seconds to retrieve this data.\")\n",
    "\n",
    "    # And then we can read in the JSON file we've created\n",
    "\n",
    "    games = pd.read_json(\"gamesresult-\" + today_date_string + \".json\")\n",
    "\n",
    "    # Now we'll clean it up!\n",
    "    # The webcrawler pulled three blank rows for each page because of the structure of the table, so let's go ahead\n",
    "    # and filter those out. The crawler also pulled everything as lists with a single element due to the JSON, so\n",
    "    # we'll be checking the length of each element to see if there's anything inside, if not we'll toss it.\n",
    "\n",
    "    games = games[~(games[\"title\"].str.len() == 0)]\n",
    "\n",
    "    # Next we'll need to convert our single-element lists to the elements themselves, so we'll select the first element\n",
    "    # in each list to be the actual value we want in each cell.\n",
    "\n",
    "    for column in ['console', 'critic_score', 'developer', 'img', 'jp_sales',\n",
    "           'last_update', 'na_sales', 'other_sales', 'pal_sales', 'publisher',\n",
    "           'release_date', 'title', 'total_sales', 'total_shipped', 'user_score',\n",
    "           'vg_score']:\n",
    "        games[column] = games[column].apply(lambda x : x[0])\n",
    "\n",
    "    # There are also some trailing spaces on some of the columns, so let's go ahead and trim those off with str.strip()\n",
    "\n",
    "    games = games.apply(lambda x : x.str.strip())\n",
    "\n",
    "    # If we hadn't done the previous step, this wouldn't have worked, but now we can convert all \"N/A\" strings in the\n",
    "    # dataset to numpy nan values.\n",
    "\n",
    "    games = games.replace(\"N/A\", np.nan)\n",
    "\n",
    "    # Now we'll start cleaning the individual columns. I'll first write a function to clean each numerical column, since\n",
    "    # they'll all follow the same general rules.\n",
    "\n",
    "    def clean_nums(column, dataframe):\n",
    "        dataframe[column] = dataframe[column].str.strip(\"m\") # This will strip the \"m\" off the end of each string if it's there\n",
    "        dataframe[column] = dataframe[column].apply(lambda x : float(x)) # This will turn all the values from strings to floats\n",
    "\n",
    "    # Now we'll just apply it to all of the sales columns\n",
    "\n",
    "    sales_columns = [\"na_sales\",\n",
    "                     \"jp_sales\",\n",
    "                     \"pal_sales\",\n",
    "                     \"other_sales\",\n",
    "                     \"total_sales\",\n",
    "                     \"total_shipped\",\n",
    "                     \"vg_score\",\n",
    "                     \"user_score\",\n",
    "                     \"critic_score\"]\n",
    "\n",
    "    for column in sales_columns:\n",
    "        clean_nums(column, games)\n",
    "\n",
    "    # Next we'll need to clean up the dates which are in string format. We'll use some regex to extract the information\n",
    "    # and then convert to datetime objects.\n",
    "\n",
    "    day_pat = r\"([0-9]{2})(?=[a-z]{2})\" # A regex pattern to select only the numerical day of the month\n",
    "    month_pat = r\"([A-Z][a-z]{2})\" # A regex pattern to select only the string abbreviated month\n",
    "    year_pat = r\"([0-9]{2}(?![a-z]{2}))\" # A regex pattern to select only the numerical year\n",
    "\n",
    "    # And here is a month string to month integer translation dictionary.\n",
    "\n",
    "    month_to_num = {'Sep' : 9,\n",
    "                    'Jul' : 7,\n",
    "                    'Oct' : 10,\n",
    "                    'Mar' : 3,\n",
    "                    'Dec' : 12,\n",
    "                    'Feb' : 2,\n",
    "                    'Nov' : 11,\n",
    "                    'Jun' : 6,\n",
    "                    'Aug' : 8,\n",
    "                    'May' : 5,\n",
    "                    'Apr' : 4,\n",
    "                    'Jan' : 1\n",
    "                    }\n",
    "\n",
    "    # Now we're actually ready to create the function to clean the dates.\n",
    "\n",
    "    def clean_dates(text):\n",
    "        global day_pat\n",
    "        global month_pat\n",
    "        global year_pat\n",
    "        global month_to_num\n",
    "        if text is np.nan:\n",
    "            return text\n",
    "\n",
    "        day = int((re.search(day_pat, text).group(1))) # This one is the easiest, we extract the numbers and convert to integer\n",
    "        month = month_to_num[(re.search(month_pat, text).group(1))] # This one we use the month string to look up the integer in our dictionary\n",
    "        year = (re.search(year_pat, text).group(1)) # Year will require a little more work as we need to fill in the first two digits\n",
    "\n",
    "        # The best we can do here is see how high the first integer is, and if it's greater than seven, we can safely assume\n",
    "        # (for now) that it was from the nineties. Otherwise, it's in the 2000s. That will be alright before 2070 in which case\n",
    "        # we've got a classic y2k situation ready to ruin us.\n",
    "\n",
    "        if int(year[0]) >= 7:\n",
    "            year = int(\"19\" + year)\n",
    "        else:\n",
    "            year = int(\"20\" + year)\n",
    "\n",
    "        return(datetime.datetime(year, month, day))\n",
    "\n",
    "    # We will apply the date cleanup function across our two date columns.\n",
    "\n",
    "    for column in [\"last_update\", \"release_date\"]:\n",
    "        games[column] = games[column].apply(lambda x : clean_dates(x))\n",
    "\n",
    "    # A quick replacement of the +'s used in the url for genre to make our genre column more readable\n",
    "\n",
    "    games[\"genre\"] = games[\"genre\"].str.replace(\"+\", \" \")\n",
    "\n",
    "    # In exploring the data, I noticed there were a few dates being used\n",
    "    # as placeholders. For the longevity of this code, I will not exclude\n",
    "    # 12/31/2020 and 12/31/2021, but any user using this code before these dates\n",
    "    # should be careful to not rely on them, and explore to see if they are\n",
    "    # still being used as placeholders. However, 1/1/1970 is used as a\n",
    "    # placeholder, and is the only date to show up in 1970, thus I can replace\n",
    "    # these dates with a nan value while keeping our mind at ease.\n",
    "\n",
    "    games.loc[games[\"release_date\"].dt.year == 1970, \"release_date\"] = np.nan \n",
    "\n",
    "    # Finally, let's reorder the columns in a way that makes the most sense.\n",
    "\n",
    "    games = games[[\"img\", \"title\", \"console\", \"genre\", \"publisher\", \"developer\", \"vg_score\", \"critic_score\", \"user_score\", \"total_shipped\", \"total_sales\", \"na_sales\", \"jp_sales\", \"pal_sales\", \"other_sales\", \"release_date\", \"last_update\"]]\n",
    "\n",
    "    # And now we have a delightful little clean dataframe of videogames that's as current as possible!\n",
    "\n",
    "    games.to_csv(\"vgchartz-\" + today_date_string +\".csv\", index=False)\n",
    "\n",
    "    print(str(games[\"title\"].count()) + \" game records retrieved.\" )\n",
    "    print(\"File saved as vgchartz-\" + today_date_string + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0f7a39f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'proxies_gen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_76992/1032101786.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0muser_agent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate_user_agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mproxies_gen\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_proxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_proxies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'proxies_gen'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, element\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import unidecode\n",
    "from user_agent import generate_user_agent\n",
    "from proxies_gen import get_proxies, test_proxies\n",
    "from itertools import cycle\n",
    "from lxml.html import fromstring\n",
    "from multiprocessing import Pool, cpu_count  # This is a thread-based Pool\n",
    "from requests.exceptions import ConnectionError, Timeout, ProxyError, RequestException\n",
    "from urllib3.exceptions import ProtocolError\n",
    "import sys\n",
    "import os\n",
    "sys.setrecursionlimit(10000)  # need to optimize code.\n",
    "proxy_enabled = True\n",
    "\n",
    "\n",
    "def parse_games(game_tags):\n",
    "    \"\"\"\n",
    "    parse the games table on current page\n",
    "    parameters:\n",
    "    game_tags: games tags after reading the html page\n",
    "    df: the dataframe where we will store the games\n",
    "    \"\"\"\n",
    "    global rec_count\n",
    "    global df\n",
    "    for tag in game_tags:\n",
    "        game = {}\n",
    "        game[\"Name\"] = \" \".join(tag.string.split())\n",
    "        #print(rec_count+1, 'Fetch Data for game', unidecode.unidecode(game['Name']))\n",
    "\n",
    "        data = tag.parent.parent.find_all(\"td\")\n",
    "        if data:\n",
    "            game[\"Rank\"] = np.int32(data[0].string)\n",
    "            game[\"img_url\"] = data[1].a.img.get('src')\n",
    "            game[\"url\"] = data[2].a.get('href')\n",
    "            if len(game[\"Name\"].split(\"/\")) > 1:\n",
    "                # replace accented chars with ascii\n",
    "                game[\"basename\"] = unidecode.unidecode(\n",
    "                    game['Name'].strip().split('/')[0].strip().replace(' ', '-'))\n",
    "            else:\n",
    "                game[\"basename\"] = game[\"url\"].rsplit('/', 2)[1]\n",
    "            game[\"Platform\"] = data[3].img.get('alt')\n",
    "            game[\"Publisher\"] = data[4].get_text().strip()\n",
    "            game[\"Developer\"] = data[5].get_text().strip()\n",
    "            game[\"Vgchartzscore\"] = data[6].get_text().strip()\n",
    "            game[\"Critic_Score\"] = float(\n",
    "                data[7].string) if not data[7].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"User_Score\"] = float(\n",
    "                data[8].string) if not data[8].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"Total_Shipped\"] = float(\n",
    "                data[9].string[:-1]) if not data[9].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"Global_Sales\"] = float(\n",
    "                data[10].string[:-1]) if not data[10].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"NA_Sales\"] = float(\n",
    "                data[11].string[:-1]) if not data[11].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"PAL_Sales\"] = float(\n",
    "                data[12].string[:-1]) if not data[12].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"JP_Sales\"] = float(\n",
    "                data[13].string[:-1]) if not data[13].string.startswith(\"N/A\") else np.nan\n",
    "            game[\"Other_Sales\"] = float(\n",
    "                data[14].string[:-1]) if not data[14].string.startswith(\"N/A\") else np.nan\n",
    "            year = data[15].string.split()[-1]\n",
    "            if year.startswith('N/A'):\n",
    "                game[\"Year\"] = 'N/A'\n",
    "            else:\n",
    "                if int(year) >= 70:\n",
    "                    year_to_add = np.int32(\"19\" + year)\n",
    "                else:\n",
    "                    year_to_add = np.int32(\"20\" + year)\n",
    "                game[\"Year\"] = year_to_add\n",
    "            game[\"Last_Update\"] = data[16].get_text().strip()\n",
    "            game['Genre'] = 'N/A'\n",
    "            game['ESRB_Rating'] = 'N/A'\n",
    "            game['status'] = 0\n",
    "            df = df.append(game, ignore_index=True)\n",
    "        rec_count += 1\n",
    "\n",
    "\n",
    "def parse_genre_esrb(df):\n",
    "    \"\"\"loads every game's url to get genre and esrb rating\"\"\"\n",
    "    headers = {'User-Agent': generate_user_agent(\n",
    "        device_type='desktop', os=('mac', 'linux'))}\n",
    "    proxy = {}\n",
    "    if proxy_enabled:\n",
    "        #print(\"\\n******getting list of proxies and testing them******'\\n\")\n",
    "        # this an api call which returns a list of working proxies that get checked evrey 15 minutes\n",
    "        proxies = cycle(get_proxies(5))\n",
    "        proxy = next(proxies)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            game_page = requests.get(df.at[index, 'url'], headers=headers, proxies={\"http\": proxy, \"https\": proxy}, timeout=5)\n",
    "            if game_page.status_code == 200:\n",
    "                sub_soup = BeautifulSoup(game_page.text, \"lxml\")\n",
    "                # again, the info box is inconsistent among games so we\n",
    "                # have to find all the h2 and traverse from that to the genre\n",
    "                gamebox = sub_soup.find(\"div\", {\"id\": \"gameGenInfoBox\"})\n",
    "                h2s = gamebox.find_all('h2')\n",
    "                # make a temporary tag here to search for the one that contains\n",
    "                # the word \"Genre\"\n",
    "                temp_tag = element.Tag\n",
    "                for h2 in h2s:\n",
    "                    if h2.string == 'Genre':\n",
    "                        temp_tag = h2\n",
    "                df.loc[index, 'Genre'] = temp_tag.next_sibling.string\n",
    "\n",
    "                # find the ESRB rating\n",
    "                game_rating = gamebox.find('img').get('src')\n",
    "                if 'esrb' in game_rating:\n",
    "                    df.loc[index, 'ESRB_Rating'] = game_rating.split(\n",
    "                        '_')[1].split('.')[0].upper()\n",
    "                # we successfuly got the genre and rating\n",
    "                df.loc[index, 'status'] = 1\n",
    "                #print('Successfully scraped genre and rating for :', df.at[index, 'Name'])\n",
    "\n",
    "        except(ProxyError):\n",
    "            proxy = next(proxies)\n",
    "\n",
    "        except (ConnectionError, Timeout, ProtocolError, TimeoutError):\n",
    "            #print('Something went wrong while connecting to', df.at[index, 'Name'], 'url, will try again later')\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            #print('different error occurred while connecting, will pass')\n",
    "            continue\n",
    "        # wait for 1 seconds between every call,\n",
    "        # we do not want to get blocked or abuse the server\n",
    "        time.sleep(1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def retry_game(df):\n",
    "    \"\"\"try to scrape the missing data again\"\"\"\n",
    "    return parse_genre_esrb(df)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def process_games(df):\n",
    "        failed_games = len(df[df['status'] == 0])\n",
    "        NUM_WORKERS = cpu_count() * 2\n",
    "        df_subsets = np.array_split(df[df['status'] == 0], NUM_WORKERS)\n",
    "        #update num_workers\n",
    "        df_subsets = [i for i in df_subsets if len(i) != 0]\n",
    "        if len(df_subsets) != 0:\n",
    "            NUM_WORKERS = len(df_subsets)# we don't want to have a worker for empty subsets\n",
    "            pool = Pool(processes=NUM_WORKERS)\n",
    "            results = pool.map(retry_game, df_subsets)\n",
    "            try:\n",
    "                df_updated = pd.concat(results)\n",
    "                df = pd.concat([df[df['status'] == 1], df_updated])\n",
    "            except: \n",
    "                print('error occurred while joining dataframe')\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        return df\n",
    "\n",
    "    rec_count = 0\n",
    "    start_time = time.time()\n",
    "    current_time = time.time()\n",
    "    crashed_tag = 'before_crashing_'\n",
    "    exists = [s for s in os.listdir() if crashed_tag in s]\n",
    "    if exists:\n",
    "        print(\"found a data saved from a crash, will continue on it\")\n",
    "        csvfilename = exists[0].replace(crashed_tag, '')\n",
    "        df = pd.read_csv(exists[0])\n",
    "        rec_count = df['Rank'].max()\n",
    "        page = int(rec_count/1000) + 1 # because we already scraped current \n",
    "        df = process_games(df)\n",
    "    else:\n",
    "        csvfilename = \"vgsales-\" + time.strftime(\"%Y-%m-%d_%H_%M_%S\") + \".csv\"\n",
    "\n",
    "    # initialize a panda dataframe to store all games with the following columns:\n",
    "    # rank, name, img-url, vgchartz score, genre, ESRB rating, platform, developer,\n",
    "    # publisher, release year, critic score, user score, na sales, pal sales,\n",
    "    # jp sales, other sales, total sales, total shipped, last update, url, status\n",
    "    # last two columns for debugging\n",
    "    if not exists:\n",
    "        df = pd.DataFrame(columns=[\n",
    "            'Rank', 'Name', 'basename', 'Genre', 'ESRB_Rating', 'Platform', 'Publisher',\n",
    "            'Developer', 'VGChartz_Score', 'Critic_Score', 'User_Score',\n",
    "            'Total_Shipped', 'Global_Sales', 'NA_Sales', 'PAL_Sales', 'JP_Sales',\n",
    "            'Other_Sales', 'Year', 'Last_Update', 'url', 'status'])\n",
    "\n",
    "    urlhead = 'http://www.vgchartz.com/games/games.php?page='\n",
    "    urltail = '&results=1000&name=&console=&keyword=&publisher=&genre=&order=Sales&ownership=Both'\n",
    "    urltail += '&banner=Both&showdeleted=&region=All&goty_year=&developer='\n",
    "    urltail += '&direction=DESC&showtotalsales=1&shownasales=1&showpalsales=1&showjapansales=1'\n",
    "    urltail += '&showothersales=1&showpublisher=1&showdeveloper=1&showreleasedate=1&showlastupdate=1'\n",
    "    urltail += '&showvgchartzscore=1&showcriticscore=1&showuserscore=1&showshipped=1&alphasort=&showmultiplat=Yes&showgenre=1'\n",
    "\n",
    "    # get the number of pages\n",
    "    vglink = requests.get('http://www.vgchartz.com/gamedb/').text\n",
    "    x = fromstring(vglink).xpath(\n",
    "        \"//th[@colspan='3']/text()\")[0].split('(', 1)[1].split(')')[0]\n",
    "    pages = int(x.split(',')[0])\n",
    "\n",
    "    if not exists: page = 1\n",
    "    while True:\n",
    "        if page > pages:\n",
    "            break\n",
    "        try:\n",
    "            proxy = get_proxies(1)[0]\n",
    "            headers = {'User-Agent': generate_user_agent(\n",
    "                device_type='desktop', os=('mac', 'linux'))}\n",
    "            surl = urlhead + str(page) + urltail\n",
    "            r = requests.get(surl, headers=headers, proxies={\n",
    "                            'http': proxy, 'https': proxy}, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                soup = BeautifulSoup(r.text, 'lxml')\n",
    "                print(\"******Scraping page \" + str(page) + \"******'\\n\")\n",
    "\n",
    "                # vgchartz website is really weird so we have to search for\n",
    "                # <a> tags with game urls\n",
    "                game_tags = list(filter(\n",
    "                    lambda x: x.attrs['href'].startswith('http://www.vgchartz.com/game/'), soup.find_all(\"a\")))[10:]\n",
    "                # discard the first 10 elements because those\n",
    "                # links are in the navigation bar\n",
    "\n",
    "                parse_games(game_tags)\n",
    "                page += 1\n",
    "                print('\\n******begin scraping for Genre and Rating******\\n')\n",
    "                df = process_games(df)\n",
    "\n",
    "        except (ConnectionError, Timeout, ProxyError, RequestException, ProtocolError, TimeoutError):\n",
    "            print('Something went wrong while connecting to page: ',\n",
    "                page, ', will try again later')\n",
    "            #proxy = get_proxies(1)\n",
    "            time.sleep(10)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"something went wrong! We're on page: \" +\n",
    "                str(page) + '\\nSaving successfully crawled data')\n",
    "            print(\"Exception: \", e)\n",
    "            df.to_csv(crashed_tag + csvfilename, sep=\",\",\n",
    "                    encoding='utf-8', index=False)\n",
    "            raise e\n",
    "\n",
    "\n",
    "    failed_games = len(df[df['status'] == 0])\n",
    "    print(\"******Finished scraping games, will try to scrape missing data******\")\n",
    "    # 36 hours max, should be enough to scrape everything\n",
    "    t_end = start_time + 60 * 60 * 36\n",
    "    while True:\n",
    "        try:\n",
    "            df = process_games(df)\n",
    "            failed_games = len(df[df['status'] == 0])\n",
    "            if failed_games == 0 or time.time() > t_end:\n",
    "                break\n",
    "            #print('Number of not scraped yet:', failed_games, '\\n')\n",
    "            time.sleep(10)  # wait for 10 seconds for the server to recover?\n",
    "        except Exception as e:\n",
    "            print(\"something went wrong! We're on page: \" + str(page) + '\\nSaving successfully crawled data')\n",
    "            print(\"Exception: \", e)\n",
    "            df.to_csv(crashed_tag + csvfilename, sep=\",\",\n",
    "                    encoding='utf-8', index=False)\n",
    "            raise e\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Scraped\", rec_count, \"games in\", round(elapsed_time/60, 2), \"minutes.\")\n",
    "\n",
    "    # select only these columns in the final dataset\n",
    "    df = df.sort_values(by=['Rank'])\n",
    "    df.to_csv('complete-vgchartz.csv', sep=\",\", encoding='utf-8', index=False)\n",
    "    df_final = df[[\n",
    "        'Rank', 'Name', 'Platform', 'Year_of_Release', 'Genre', 'Rating',\n",
    "        'Publisher', 'Developer', 'Critic_Score', 'User_Score',\n",
    "        'Global_Sales', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']]\n",
    "\n",
    "    df_final.to_csv(csvfilename, sep=\",\", encoding='utf-8', index=False)\n",
    "    print(\"Wrote scraper data to\", csvfilename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e52525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
